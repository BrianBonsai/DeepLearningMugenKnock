{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "HRED_pytorch_sand.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyoyo-yo/DeepLearningMugenKnock/blob/master/pytorch/HRED_pytorch_sand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0H8bE7gkhEV"
      },
      "source": [
        "# HRED\n",
        "\n",
        "元論文 : Attention if All You Need https://arxiv.org/abs/1706.03762 (2017)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIRpEHgCkoE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bac210a-a9ae-45cc-a47e-574ca37ec33f"
      },
      "source": [
        "!pip install numpy matplotlib opencv-python torch torchvision torchsummary pandas easydict"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (1.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.7)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab___Yzwk1hq"
      },
      "source": [
        "# Ginza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjvQdntDk3ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e5e61a-309e-41f9-b22b-e1a276d503f0"
      },
      "source": [
        "!pip install ginza"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ginza in /usr/local/lib/python3.6/dist-packages (4.0.5)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from ginza) (2.3.2)\n",
            "Requirement already satisfied: SudachiPy>=0.4.9; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from ginza) (0.4.9)\n",
            "Requirement already satisfied: SudachiDict-core>=20200330; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from ginza) (20200722)\n",
            "Requirement already satisfied: ja-ginza<4.1.0,>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ginza) (4.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (7.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (2.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (50.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (4.41.1)\n",
            "Requirement already satisfied: dartsclone~=0.9.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy>=0.4.9; python_version >= \"3.5\"->ginza) (0.9.0)\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy>=0.4.9; python_version >= \"3.5\"->ginza) (2.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.2->ginza) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza) (2020.6.20)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.9.0->SudachiPy>=0.4.9; python_version >= \"3.5\"->ginza) (0.29.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.2->ginza) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjGBSMOxk7sS"
      },
      "source": [
        "import pkg_resources, imp\n",
        "imp.reload(pkg_resources)\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('ja_ginza')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEn0fNCVk98x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7e752d-fd08-4965-f06d-5601b60edab0"
      },
      "source": [
        "# test\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('ja_ginza')\n",
        "doc = nlp('あのラーメン屋にはよく行く。美味しいんだ。')\n",
        "\n",
        "for sent in doc.sents:\n",
        "    for token in sent:\n",
        "        info = [\n",
        "            token.i,         # トークン番号\n",
        "            token.orth_,     # テキスト\n",
        "            #token._.reading, # 読みカナ\n",
        "            token.lemma_,    # 基本形\n",
        "            token.pos_,      # 品詞\n",
        "            token.tag_,      # 品詞詳細\n",
        "            #token._.inf      # 活用情報\n",
        "        ]\n",
        "        print(info)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 'あの', 'あの', 'DET', '連体詞']\n",
            "[1, 'ラーメン', 'ラーメン', 'NOUN', '名詞-普通名詞-一般']\n",
            "[2, '屋', '屋', 'NOUN', '接尾辞-名詞的-一般']\n",
            "[3, 'に', 'に', 'ADP', '助詞-格助詞']\n",
            "[4, 'は', 'は', 'ADP', '助詞-係助詞']\n",
            "[5, 'よく', 'よく', 'ADV', '副詞']\n",
            "[6, '行く', '行く', 'VERB', '動詞-非自立可能']\n",
            "[7, '。', '。', 'PUNCT', '補助記号-句点']\n",
            "[8, '美味しい', '美味しい', 'ADJ', '形容詞-一般']\n",
            "[9, 'ん', 'ん', 'SCONJ', '助詞-準体助詞']\n",
            "[10, 'だ', 'だ', 'AUX', '助動詞']\n",
            "[11, '。', '。', 'PUNCT', '補助記号-句点']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6j-kiToPUyo"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NnIwttOjHJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336d416d-a5f6-4232-a80c-fe1cfae933b3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rFAds04jPnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b258eb-2cf0-44e7-ed62-70bcfbc29e32"
      },
      "source": [
        "from glob import glob\n",
        "\n",
        "glob('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/*_original.txt')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_hanayome_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_gasorin_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_sougiya_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_sanpo_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_ijime_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_syokumushitsumon_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_momotaro_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_ryokou_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_anpanman_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_keisatsu_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_speech_original.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_soushiki_original.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awXNxB6-jPqy"
      },
      "source": [
        "def get_corpus(fname):\n",
        "    corpus = []\n",
        "\n",
        "    with open(fname, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip()\n",
        "            _corpus = []\n",
        "            for sent in nlp(line).sents:\n",
        "                for token in sent:\n",
        "                    _corpus.append(token.orth_)\n",
        "\n",
        "            corpus = list(set(corpus) | set(_corpus))\n",
        "    corpus.sort()\n",
        "    return corpus"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owHqIBMyjPt-"
      },
      "source": [
        "# sample\n",
        "corpus = get_corpus('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_hanayome_original.txt')\n",
        "corpus = ['<UNKNOWN>'] + corpus"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPdw6l74tYMW"
      },
      "source": [
        "def read_data(fname, corpus):\n",
        "    Xs = []\n",
        "    with open(fname, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip()\n",
        "            _Xs = [corpus.index('<SOS>')]\n",
        "            for sent in nlp(line).sents:\n",
        "                for token in sent:\n",
        "                    w = token.orth_\n",
        "\n",
        "                    if w in corpus:\n",
        "                        ind = corpus.index(w)\n",
        "                    else:\n",
        "                        ind = corpus.index('<UNKNOWN>')\n",
        "                    _Xs.append(ind)\n",
        "            _Xs.append(corpus.index('<EOS>'))\n",
        "            Xs.append(_Xs)\n",
        "\n",
        "    return Xs"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ-z54NTvcvV"
      },
      "source": [
        "def get_data(data, data_n=None):\n",
        "    data_n = len(data) if data_n is None else data_n\n",
        "    Xs = []\n",
        "    for i in range(0, len(data) - data_n):\n",
        "        _Xs = []\n",
        "        for j in range(data_n):\n",
        "            _Xs.append(data[i + j])\n",
        "        Xs.append(_Xs)\n",
        "    return Xs"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY1gx9FduRvu"
      },
      "source": [
        "# sample\n",
        "# data = read_data('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_hanayome_original.txt', corpus)\n",
        "# get_data(data, data_n=HRED_SESSION)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjoKf_PPQdl"
      },
      "source": [
        "# get corpus\n",
        "corpus = []\n",
        "for fpath in glob('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/*_original.txt'):\n",
        "    _corpus = get_corpus(fpath)\n",
        "    corpus = list(set(corpus) | set(_corpus))\n",
        "\n",
        "corpus.sort()\n",
        "corpus = ['<SOS>', '<EOS>', '<UNKNOWN>'] + corpus"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9i2CqZ0vTQe"
      },
      "source": [
        "# get training data\n",
        "data_Xs = []\n",
        "\n",
        "HRED_SESSION = 5\n",
        "\n",
        "for fpath in glob('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/*_original.txt'):\n",
        "    data = read_data(fpath, corpus)\n",
        "    _data_Xs = get_data(data, data_n=5)\n",
        "    data_Xs += _data_Xs"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ64UhTzkhEW"
      },
      "source": [
        "# Import and Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JWBAlBskhEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170ccab1-c135-4232-84ca-3075b390bdc0"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from easydict import EasyDict\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "#---\n",
        "# config\n",
        "#---\n",
        "cfg = EasyDict()\n",
        "\n",
        "cfg.CORPUS1_NUM = len(corpus)\n",
        "cfg.CORPUS2_NUM =  len(corpus)\n",
        "\n",
        "# Seq2seq config\n",
        "cfg.SEQ2SEQ_MAX_LENGTH = 1000 # decoder max output length\n",
        "cfg.SEQ2SEQ_TRAIN_FORCE_PROB = 0.5 # train input is forced to gt with this probability\n",
        "cfg.SEQ2SEQ_NEXT_WORD_SELECTION = 'prob' # prob, argmax\n",
        "cfg.SEQ2SEQ_RNN_DIM = 512\n",
        "cfg.SEQ2SEQ_USE_RNN_BD = True # use bidirectional RNN\n",
        "\n",
        "cfg.SEQ2SEQ_E_ATTENTION = False\n",
        "cfg.SEQ2SEQ_E_ATTENTION_TIME = 2  # Hopping if > 1\n",
        "cfg.SEQ2SEQ_E_DIM = 64\n",
        "cfg.SEQ2SEQ_E_ATTENTION_DIM = 64\n",
        "cfg.SEQ2SEQ_E_DROPOUT = 0.2\n",
        "cfg.SEQ2SEQ_E_USE_SELF_ATTENTION = True # self attention of Encoder\n",
        "cfg.SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION = True # use source target attention\n",
        "cfg.SEQ2SEQ_E_MULTIHEAD_ATTENTION_N = 8 # Multi head attention\n",
        "cfg.SEQ2SEQ_E_USE_FFN = True # Feed forward network\n",
        "cfg.SEQ2SEQ_E_FFN_DIM = 512\n",
        "cfg.SEQ2SEQ_E_USE_PE = True # Positional encoding\n",
        "\n",
        "cfg.SEQ2SEQ_D_ATTENTION = False\n",
        "cfg.SEQ2SEQ_D_ATTENTION_TIME = 2  # Hopping if > 1\n",
        "cfg.SEQ2SEQ_D_DIM = 64\n",
        "cfg.SEQ2SEQ_D_ATTENTION_DIM = 64\n",
        "cfg.SEQ2SEQ_D_DROPOUT = 0.2\n",
        "cfg.SEQ2SEQ_D_USE_SELF_ATTENTIION = True # self attention of Decoder\n",
        "cfg.SEQ2SEQ_D_USE_SOURCE_TARGET_ATTENTION = True # use source target attention\n",
        "cfg.SEQ2SEQ_D_MULTIHEAD_ATTENTION_N = 8 # Multi head attention\n",
        "cfg.SEQ2SEQ_D_USE_FFN = True # Feed forward network\n",
        "cfg.SEQ2SEQ_D_FFN_DIM = 512\n",
        "cfg.SEQ2SEQ_D_USE_PE = True # Positional encoding\n",
        "\n",
        "cfg.HRED_HIDDEN_DIM = 512 # d_s in original paper\n",
        "\n",
        "cfg.CHANNEL_AXIS = 1 # 1 ... [mb, c, h, w], 3 ... [mb, h, w, c]\n",
        "\n",
        "cfg.GPU = True\n",
        "cfg.DEVICE_TYPE = 'cuda' if cfg.GPU and torch.cuda.is_available() else 'cpu'\n",
        "cfg.DEVICE = torch.device(cfg.DEVICE_TYPE)\n",
        "\n",
        "# train\n",
        "cfg.TRAIN = EasyDict()\n",
        "cfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n",
        "\n",
        "cfg.PREFIX = 'Seq2seq-Attention'\n",
        "cfg.TRAIN.MODEL_E_SAVE_PATH = 'models/' + cfg.PREFIX + '_E_{}.pt'\n",
        "cfg.TRAIN.MODEL_D_SAVE_PATH = 'models/' + cfg.PREFIX + '_D_{}.pt'\n",
        "cfg.TRAIN.MODEL_SAVE_INTERVAL = 50\n",
        "cfg.TRAIN.ITERATION = 50\n",
        "cfg.TRAIN.MINIBATCH = 1\n",
        "cfg.TRAIN.OPTIMIZER_E = torch.optim.Adam\n",
        "cfg.TRAIN.LEARNING_PARAMS_E = {'lr' : 0.01, 'betas' : (0., 0.9)}\n",
        "cfg.TRAIN.OPTIMIZER_D = torch.optim.Adam\n",
        "cfg.TRAIN.LEARNING_PARAMS_D = {'lr' : 0.01, 'betas' : (0., 0.9)}\n",
        "cfg.TRAIN.OPTIMIZER_H = torch.optim.Adam\n",
        "cfg.TRAIN.LEARNING_PARAMS_H = {'lr' : 0.01, 'betas' : (0., 0.9)}\n",
        "cfg.TRAIN.LOSS_FUNCTION = torch.nn.NLLLoss()\n",
        "\n",
        "cfg.TRAIN.DATA_PATH = '/content/drive/My Drive/Colab Notebooks/Dataset/train/images/'\n",
        "cfg.TRAIN.DATA_HORIZONTAL_FLIP = True # data augmentation : holizontal flip\n",
        "cfg.TRAIN.DATA_VERTICAL_FLIP = True # data augmentation : vertical flip\n",
        "cfg.TRAIN.DATA_ROTATION = 1 # data augmentation : rotation False, or integer\n",
        "\n",
        "cfg.TRAIN.LEARNING_PROCESS_RESULT_SAVE = True\n",
        "cfg.TRAIN.LEARNING_PROCESS_RESULT_INTERVAL = 200\n",
        "cfg.TRAIN.LEARNING_PROCESS_RESULT_IMAGE_PATH = 'result/' + cfg.PREFIX + '_result_{}.jpg'\n",
        "cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH = 'result/' + cfg.PREFIX + '_loss.txt'\n",
        "\n",
        "\n",
        "# test\n",
        "cfg.TEST = EasyDict()\n",
        "cfg.TEST.MODEL_E_PATH = cfg.TRAIN.MODEL_E_SAVE_PATH.format('final')\n",
        "cfg.TEST.MODEL_D_PATH = cfg.TRAIN.MODEL_D_SAVE_PATH.format('final')\n",
        "cfg.TEST.DATA_PATH = '/content/drive/My Drive/Colab Notebooks/Dataset/test/images/'\n",
        "cfg.TEST.MINIBATCH = 10\n",
        "cfg.TEST.ITERATION = 2\n",
        "cfg.TEST.RESULT_SAVE = False\n",
        "cfg.TEST.RESULT_IMAGE_PATH = 'result/' + cfg.PREFIX + '_result_{}.jpg'\n",
        "\n",
        "# random seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "# make model save directory\n",
        "def make_dir(path):\n",
        "    if '/' in path:\n",
        "        model_save_dir = '/'.join(path.split('/')[:-1])\n",
        "        os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "make_dir(cfg.TRAIN.MODEL_D_SAVE_PATH)\n",
        "make_dir(cfg.TRAIN.LEARNING_PROCESS_RESULT_IMAGE_PATH)\n",
        "make_dir(cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH)\n",
        "\n",
        "pprint(cfg)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'CHANNEL_AXIS': 1,\n",
            " 'CORPUS1_NUM': 2156,\n",
            " 'CORPUS2_NUM': 2156,\n",
            " 'DEVICE': device(type='cpu'),\n",
            " 'DEVICE_TYPE': 'cpu',\n",
            " 'GPU': True,\n",
            " 'HRED_HIDDEN_DIM': 512,\n",
            " 'PREFIX': 'Seq2seq-Attention',\n",
            " 'SEQ2SEQ_D_ATTENTION': False,\n",
            " 'SEQ2SEQ_D_ATTENTION_DIM': 64,\n",
            " 'SEQ2SEQ_D_ATTENTION_TIME': 2,\n",
            " 'SEQ2SEQ_D_DIM': 64,\n",
            " 'SEQ2SEQ_D_DROPOUT': 0.2,\n",
            " 'SEQ2SEQ_D_FFN_DIM': 512,\n",
            " 'SEQ2SEQ_D_MULTIHEAD_ATTENTION_N': 8,\n",
            " 'SEQ2SEQ_D_USE_FFN': True,\n",
            " 'SEQ2SEQ_D_USE_PE': True,\n",
            " 'SEQ2SEQ_D_USE_SELF_ATTENTIION': True,\n",
            " 'SEQ2SEQ_D_USE_SOURCE_TARGET_ATTENTION': True,\n",
            " 'SEQ2SEQ_E_ATTENTION': False,\n",
            " 'SEQ2SEQ_E_ATTENTION_DIM': 64,\n",
            " 'SEQ2SEQ_E_ATTENTION_TIME': 2,\n",
            " 'SEQ2SEQ_E_DIM': 64,\n",
            " 'SEQ2SEQ_E_DROPOUT': 0.2,\n",
            " 'SEQ2SEQ_E_FFN_DIM': 512,\n",
            " 'SEQ2SEQ_E_MULTIHEAD_ATTENTION_N': 8,\n",
            " 'SEQ2SEQ_E_USE_FFN': True,\n",
            " 'SEQ2SEQ_E_USE_PE': True,\n",
            " 'SEQ2SEQ_E_USE_SELF_ATTENTION': True,\n",
            " 'SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION': True,\n",
            " 'SEQ2SEQ_MAX_LENGTH': 1000,\n",
            " 'SEQ2SEQ_NEXT_WORD_SELECTION': 'prob',\n",
            " 'SEQ2SEQ_RNN_DIM': 512,\n",
            " 'SEQ2SEQ_TRAIN_FORCE_PROB': 0.5,\n",
            " 'SEQ2SEQ_USE_RNN_BD': True,\n",
            " 'TEST': {'DATA_PATH': '/content/drive/My Drive/Colab '\n",
            "                       'Notebooks/Dataset/test/images/',\n",
            "          'ITERATION': 2,\n",
            "          'MINIBATCH': 10,\n",
            "          'MODEL_D_PATH': 'models/Seq2seq-Attention_D_final.pt',\n",
            "          'MODEL_E_PATH': 'models/Seq2seq-Attention_E_final.pt',\n",
            "          'RESULT_IMAGE_PATH': 'result/Seq2seq-Attention_result_{}.jpg',\n",
            "          'RESULT_SAVE': False},\n",
            " 'TRAIN': {'DATA_HORIZONTAL_FLIP': True,\n",
            "           'DATA_PATH': '/content/drive/My Drive/Colab '\n",
            "                        'Notebooks/Dataset/train/images/',\n",
            "           'DATA_ROTATION': 1,\n",
            "           'DATA_VERTICAL_FLIP': True,\n",
            "           'DISPAY_ITERATION_INTERVAL': 50,\n",
            "           'ITERATION': 50,\n",
            "           'LEARNING_PARAMS_D': {'betas': [0.0, 0.9], 'lr': 0.01},\n",
            "           'LEARNING_PARAMS_E': {'betas': [0.0, 0.9], 'lr': 0.01},\n",
            "           'LEARNING_PARAMS_H': {'betas': [0.0, 0.9], 'lr': 0.01},\n",
            "           'LEARNING_PROCESS_RESULT_IMAGE_PATH': 'result/Seq2seq-Attention_result_{}.jpg',\n",
            "           'LEARNING_PROCESS_RESULT_INTERVAL': 200,\n",
            "           'LEARNING_PROCESS_RESULT_LOSS_PATH': 'result/Seq2seq-Attention_loss.txt',\n",
            "           'LEARNING_PROCESS_RESULT_SAVE': True,\n",
            "           'LOSS_FUNCTION': NLLLoss(),\n",
            "           'MINIBATCH': 1,\n",
            "           'MODEL_D_SAVE_PATH': 'models/Seq2seq-Attention_D_{}.pt',\n",
            "           'MODEL_E_SAVE_PATH': 'models/Seq2seq-Attention_E_{}.pt',\n",
            "           'MODEL_SAVE_INTERVAL': 50,\n",
            "           'OPTIMIZER_D': <class 'torch.optim.adam.Adam'>,\n",
            "           'OPTIMIZER_E': <class 'torch.optim.adam.Adam'>,\n",
            "           'OPTIMIZER_H': <class 'torch.optim.adam.Adam'>}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfhZkkxckhEZ"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHSZpaajkhEa"
      },
      "source": [
        "class Reshape(torch.nn.Module):\n",
        "    def __init__(self, shape):\n",
        "        super(Reshape, self).__init__()\n",
        "        self.shape = shape\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.reshape(self.shape)\n",
        "\n",
        "class Permute(torch.nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super(Permute, self).__init__()\n",
        "        self.shape = args\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.permute(self.shape)\n",
        "\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, rnn_dim=64, rnn_hidden_size=1, attention_dim=64, max_length=100, \n",
        "        dropout_p=0.1, attention_time=1, use_source_target_attention=False,\n",
        "        use_self_attention=False, multiHead_attention_num=1, use_FFN=False, FFN_dim=2048, use_PE=False, use_bd=False):\n",
        "    \n",
        "        super(Encoder, self).__init__()\n",
        "        self.max_length = max_length\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "\n",
        "        # Embedding\n",
        "        self.embedding = torch.nn.Embedding(input_dim, hidden_dim)\n",
        "\n",
        "        # Positional Encoding\n",
        "        if use_PE:\n",
        "            self.pe = PE()\n",
        "\n",
        "        # Attention\n",
        "        self.attentions = []\n",
        "        if attention_time > 0:\n",
        "            for i in range(attention_time):\n",
        "                # Self Attention\n",
        "                if use_self_attention:\n",
        "                    self.attentions.append(Attention(\n",
        "                        hidden_dim=hidden_dim, memory_dim=hidden_dim, attention_dim=attention_dim, output_dim=hidden_dim,\n",
        "                        dropout_p=dropout_p, max_length=max_length, self_attention=use_self_attention, head_num=multiHead_attention_num))\n",
        "\n",
        "                # Feed Forward Network\n",
        "                if use_FFN:\n",
        "                    self.attentions.append(FFN(dim=FFN_dim, d_model=hidden_dim, dropout_p=dropout_p))\n",
        "\n",
        "        self.attentions = torch.nn.ModuleList(self.attentions)\n",
        "\n",
        "        # output GRU\n",
        "        self.gru = torch.nn.GRU(hidden_dim, rnn_dim, bidirectional=use_bd)\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden, x_memory):\n",
        "        # Embedding\n",
        "        x = self.embedding(x).view(1, 1, -1)\n",
        "        x_memory = self.embedding(x_memory).permute(1, 0, 2)\n",
        "        x_memory = x_memory.float()\n",
        "\n",
        "        # Positional Encoding\n",
        "        if hasattr(self, 'PE'):\n",
        "            x = self.pe(x)\n",
        "            x_memory = self.pe(x_memory)\n",
        "\n",
        "        # Attention\n",
        "        for layer in self.attentions:\n",
        "            x = layer(x, x_memory, x_memory)\n",
        "\n",
        "        # RNN\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        return x, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.rnn_hidden_size, 1, self.rnn_dim).to(cfg.DEVICE)\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim=64, rnn_dim=64, attention_dim=64, dropout_p=0.1,\n",
        "        attention_time=1, max_length=100, use_source_target_attention=False, use_self_attention=False,\n",
        "        multiHead_attention_num=2, use_FFN=False, FFN_dim=2048, use_PE=False, use_bd=False):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Embedding\n",
        "        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n",
        "        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "        # Positional Encoding\n",
        "        if use_PE:\n",
        "            self.pe = PE()\n",
        "\n",
        "        # Attention\n",
        "        self.attentions = []\n",
        "        if attention_time > 0:\n",
        "            for i in range(attention_time):\n",
        "                # Self Attention\n",
        "                if use_self_attention:\n",
        "                    self.attentions.append(\n",
        "                        Attention(hidden_dim=hidden_dim, memory_dim=hidden_dim, attention_dim=attention_dim, output_dim=hidden_dim,\n",
        "                        dropout_p=dropout_p, max_length=max_length, self_attention=use_self_attention, head_num=multiHead_attention_num))\n",
        "                \n",
        "                # Source Target Attention\n",
        "                if use_source_target_attention:\n",
        "                    self.attentions.append(\n",
        "                        Attention(hidden_dim=hidden_dim, memory_dim=rnn_dim * (use_bd + 1), attention_dim=attention_dim, output_dim=hidden_dim,\n",
        "                        dropout_p=dropout_p, max_length=max_length, head_num=multiHead_attention_num))\n",
        "\n",
        "                # Feed Forward Network\n",
        "                if use_FFN:\n",
        "                    self.attentions.append(FFN(dim=FFN_dim, d_model=hidden_dim, dropout_p=dropout_p))\n",
        "\n",
        "        self.attentions = torch.nn.ModuleList(self.attentions)\n",
        "\n",
        "        # output GRU\n",
        "        self.gru = torch.nn.GRU(hidden_dim, rnn_dim, bidirectional=use_bd)\n",
        "        self.out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(rnn_dim * (use_bd + 1), output_dim),\n",
        "            torch.nn.Softmax(dim=-1)\n",
        "        )\n",
        "    \n",
        "\n",
        "    def forward(self, x, hidden, x_memory_encoder, x_self_memory):\n",
        "        # Embedding\n",
        "        x = self.input_embedding(x)\n",
        "        x = self.input_embedding_dropout(x)\n",
        "\n",
        "        # Memory Embedding\n",
        "        x_self_memory = self.input_embedding(x_self_memory)#.permute(1, 0, 2)\n",
        "\n",
        "        # Positional Encoding\n",
        "        if hasattr(self, \"pe\"):\n",
        "            x = self.pe(x)\n",
        "            x_self_memory = self.pe(x_self_memory)\n",
        "\n",
        "        # Attention\n",
        "        for layer in self.attentions:\n",
        "            x = layer(x, x_memory_encoder, x_self_memory)\n",
        "\n",
        "        # output GRU\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        x = self.out(x[0])\n",
        "        return x, hidden\n",
        "\n",
        "\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, memory_dim, attention_dim, output_dim, dropout_p=0.1, max_length=100, head_num=1, self_attention=False):\n",
        "        super(Attention, self).__init__()\n",
        "        self.max_length = max_length\n",
        "        self.self_attention = self_attention\n",
        "\n",
        "        # Attention Query\n",
        "        self.Q = torch.nn.Sequential(\n",
        "            Reshape([1, -1]),\n",
        "            torch.nn.Linear(hidden_dim, attention_dim),\n",
        "            torch.nn.Dropout(dropout_p),\n",
        "            Reshape([1, 1, -1]),\n",
        "            Reshape([1, attention_dim // head_num, head_num]), # Multi head attention\n",
        "            Permute(2, 0, 1),\n",
        "        )\n",
        "        \n",
        "        # Attention Key\n",
        "        self.K = torch.nn.Sequential(\n",
        "            torch.nn.Linear(memory_dim, attention_dim),\n",
        "            torch.nn.Dropout(dropout_p),\n",
        "            Reshape([1, -1, attention_dim]),\n",
        "            Reshape([-1, attention_dim // head_num, head_num]), # Multi head attention\n",
        "            Permute(2, 1, 0)\n",
        "        )\n",
        "        \n",
        "        # Attetion Value\n",
        "        self.V = torch.nn.Sequential(\n",
        "            torch.nn.Linear(memory_dim, attention_dim),\n",
        "            torch.nn.Dropout(dropout_p),\n",
        "            Reshape([1, -1, attention_dim]),\n",
        "            Reshape([-1, attention_dim // head_num, head_num]), # Multi head attention\n",
        "            Permute(2, 0, 1),\n",
        "        )\n",
        "\n",
        "        self.out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(attention_dim, output_dim),\n",
        "            torch.nn.Dropout(dropout_p)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, memory, memory2):\n",
        "        # get Query\n",
        "        Q = self.Q(x)\n",
        "        Q *= Q.size()[-1] ** -0.5 # scaled dot product\n",
        "\n",
        "        if self.self_attention:\n",
        "            memory = memory2\n",
        "\n",
        "        # memory transforme [mb(=1), length, dim] -> [length, dim]\n",
        "        if len(memory.size()) > 2:\n",
        "            memory = memory[0]\n",
        "        \n",
        "        # get Key\n",
        "        K = self.K(memory)\n",
        "\n",
        "        QK = torch.bmm(Q, K) # get Query and Key (= attention logits)\n",
        "\n",
        "        # masking attention weight\n",
        "        any_zero = memory.sum(dim=1)\n",
        "        pad_mask = torch.ones([1, 1, self.max_length]).to(cfg.DEVICE)\n",
        "        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n",
        "\n",
        "        pad_mask = pad_mask[:, :, :QK.size()[-1]] # crop \n",
        "        QK += pad_mask * 1e-10\n",
        "        attention_weights = F.softmax(QK, dim=-1) # get attention weight\n",
        "        \n",
        "        # get Value\n",
        "        V = self.V(memory)\n",
        "        \n",
        "        # Attetion x Value\n",
        "        x = torch.bmm(attention_weights, V)\n",
        "\n",
        "        # Multi head -> one head\n",
        "        x = x.permute(1, 2, 0).reshape(1, 1, -1)\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "class FFN(torch.nn.Module):\n",
        "    def __init__(self, dim, d_model, dropout_p=0.1):\n",
        "        super(FFN, self).__init__()\n",
        "\n",
        "        self.module = torch.nn.Sequential(\n",
        "            torch.nn.Linear(d_model, dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout_p),\n",
        "            torch.nn.Linear(dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, memory_encoder, decoder):\n",
        "        return self.module(x)\n",
        "\n",
        "class PE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PE, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        mb, pos, dim = x.size()\n",
        "        pe = np.zeros_like(x.detach().cpu().numpy())\n",
        "        pos_i, dim_i = np.meshgrid(np.arange(dim), np.arange(pos))\n",
        "        pe[..., 0::2] = np.sin(pos_i[..., 0::2] / (10000 ** (2 * dim_i[..., 0::2] / dim)))\n",
        "        pe[..., 1::2] = np.cos(pos_i[..., 1::2] / (10000 ** (2 * dim_i[..., 1::2] / dim)))\n",
        "        pe = torch.tensor(pe).to(cfg.DEVICE)\n",
        "        return x + pe\n",
        "\n",
        "\n",
        "class HRED(torch.nn.Module):\n",
        "    def __init__(self, decoder_dim, hidden_dim, num_layers=1, use_bd=False):\n",
        "        super(HRED, self).__init__()\n",
        "        self.HRED_hidden_dim = hidden_dim\n",
        "        self.tensor_dim = use_bd + 1\n",
        "\n",
        "        # output GRU\n",
        "        self.gru = torch.nn.GRU(decoder_dim * (use_bd + 1), self.HRED_hidden_dim, num_layers=num_layers, bidirectional=use_bd)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        return x, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros([self.tensor_dim, 1, self.HRED_hidden_dim], device=cfg.DEVICE)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWdUenQ8khEc"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw9HD3tjkhEc"
      },
      "source": [
        "class MInibatch_Generator():\n",
        "    def __init__(self, data_size, batch_size, shuffle=True):\n",
        "        self.data_size = data_size\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.mbi = 0 # index for iteration\n",
        "        self.inds = np.arange(data_size)\n",
        "        np.random.shuffle(self.inds)\n",
        "\n",
        "    def __call__(self):\n",
        "        if self.mbi + self.batch_size > self.data_size:\n",
        "            inds = self.inds[self.mbi:]\n",
        "            if self.shuffle:\n",
        "                np.random.shuffle(self.inds)\n",
        "            inds = np.hstack((inds, self.inds[ : (self.batch_size - (self.data_size - self.mbi))]))\n",
        "            self.mbi = self.batch_size - (self.data_size - self.mbi)\n",
        "        else:\n",
        "            inds = self.inds[self.mbi : self.mbi + self.batch_size]\n",
        "            self.mbi += self.batch_size\n",
        "        return inds\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvsBhxJzkhEe"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "u93U8jihkhEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd87c0f1-c769-4be6-cd26-05f6ab28930a"
      },
      "source": [
        "# train\n",
        "def train():\n",
        "    # model\n",
        "    E = Encoder(\n",
        "        input_dim = cfg.CORPUS1_NUM, \n",
        "        hidden_dim = cfg.SEQ2SEQ_E_DIM,\n",
        "        attention_dim = cfg.SEQ2SEQ_E_ATTENTION_DIM,\n",
        "        rnn_dim = cfg.SEQ2SEQ_RNN_DIM,\n",
        "        rnn_hidden_size = cfg.SEQ2SEQ_USE_RNN_BD + 1,\n",
        "        use_bd = cfg.SEQ2SEQ_USE_RNN_BD,\n",
        "        dropout_p = cfg.SEQ2SEQ_E_DROPOUT,\n",
        "        attention_time = cfg.SEQ2SEQ_E_ATTENTION_TIME,\n",
        "        use_source_target_attention = cfg.SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION,\n",
        "        use_self_attention = cfg.SEQ2SEQ_E_USE_SELF_ATTENTION,\n",
        "        multiHead_attention_num = cfg.SEQ2SEQ_E_MULTIHEAD_ATTENTION_N,\n",
        "        use_FFN = cfg.SEQ2SEQ_E_USE_FFN,\n",
        "        FFN_dim = cfg.SEQ2SEQ_E_FFN_DIM,\n",
        "        use_PE = cfg.SEQ2SEQ_E_USE_PE,\n",
        "        max_length = cfg.SEQ2SEQ_MAX_LENGTH\n",
        "        ).to(cfg.DEVICE) \n",
        "\n",
        "    D = Decoder(\n",
        "        output_dim = cfg.CORPUS2_NUM, \n",
        "        hidden_dim = cfg.SEQ2SEQ_E_DIM,\n",
        "        rnn_dim = cfg.SEQ2SEQ_RNN_DIM,\n",
        "        use_bd = cfg.SEQ2SEQ_USE_RNN_BD,\n",
        "        attention_dim = cfg.SEQ2SEQ_E_ATTENTION_DIM,\n",
        "        dropout_p = cfg.SEQ2SEQ_E_DROPOUT,\n",
        "        attention_time = cfg.SEQ2SEQ_E_ATTENTION_TIME,\n",
        "        use_source_target_attention = cfg.SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION,\n",
        "        use_self_attention = cfg.SEQ2SEQ_E_USE_SELF_ATTENTION,\n",
        "        multiHead_attention_num = cfg.SEQ2SEQ_E_MULTIHEAD_ATTENTION_N,\n",
        "        use_FFN = cfg.SEQ2SEQ_E_USE_FFN,\n",
        "        FFN_dim = cfg.SEQ2SEQ_E_FFN_DIM,\n",
        "        use_PE = cfg.SEQ2SEQ_E_USE_PE,\n",
        "        max_length = cfg.SEQ2SEQ_MAX_LENGTH\n",
        "        ).to(cfg.DEVICE)\n",
        "\n",
        "    H = HRED(\n",
        "        decoder_dim=cfg.SEQ2SEQ_RNN_DIM,\n",
        "        hidden_dim=cfg.HRED_HIDDEN_DIM,\n",
        "        use_bd=cfg.SEQ2SEQ_USE_RNN_BD\n",
        "    ).to(cfg.DEVICE)\n",
        "\n",
        "    #summary(E, (cfg.INPUT_Z_DIM, 1, 1), device=cfg.DEVICE_TYPE)\n",
        "    #summary(D, (cfg.OUTPUT_CHANNEL, cfg.OUTPUT_HEIGHT, cfg.OUTPUT_WIDTH), device=cfg.DEVICE_TYPE)\n",
        "    \n",
        "    opt_E = cfg.TRAIN.OPTIMIZER_E(E.parameters(), **cfg.TRAIN.LEARNING_PARAMS_E)\n",
        "    opt_D = cfg.TRAIN.OPTIMIZER_D(D.parameters(), **cfg.TRAIN.LEARNING_PARAMS_D)\n",
        "    opt_H = cfg.TRAIN.OPTIMIZER_H(H.parameters(), **cfg.TRAIN.LEARNING_PARAMS_H)\n",
        "\n",
        "    list_iter = []\n",
        "    list_loss = []\n",
        "    list_accuracy = []\n",
        "\n",
        "    #dataset = MyDataset(data_dict['data1'], data_dict['data2'])\n",
        "    #dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.MINIBATCH, shuffle=True)\n",
        "\n",
        "    mb_gen = MInibatch_Generator(len(data_Xs), cfg.TRAIN.MINIBATCH)\n",
        "\n",
        "    print('training start')\n",
        "    progres_bar = ''\n",
        "\n",
        "    Xs_train = data_Xs\n",
        "    #ts_train = data_dict['data2']\n",
        "\n",
        "    for i in range(cfg.TRAIN.ITERATION):\n",
        "        idxs = mb_gen()\n",
        "        loss = 0.\n",
        "        accuracy = 0.\n",
        "        total_len = 0.\n",
        "        _Xs = [Xs_train[idx] for idx in idxs]\n",
        "        #_ts = [ts_train[idx] for idx in idxs]\n",
        "\n",
        "        # each iteration in minibatch\n",
        "        opt_E.zero_grad()\n",
        "        opt_D.zero_grad()\n",
        "        opt_H.zero_grad()\n",
        "\n",
        "        for mbi in range(cfg.TRAIN.MINIBATCH):\n",
        "            Xs_mb = _Xs[mbi]\n",
        "\n",
        "            # encode process\n",
        "            E_hidden = E.initHidden() # initialize encoder hidden\n",
        "            H_hidden = H.initHidden() # initialize hred hidden\n",
        "\n",
        "            for sess_i in range(HRED_SESSION - 1):\n",
        "                E_outputs = torch.zeros(cfg.SEQ2SEQ_MAX_LENGTH, cfg.SEQ2SEQ_RNN_DIM * (cfg.SEQ2SEQ_USE_RNN_BD + 1)).to(cfg.DEVICE)\n",
        "\n",
        "                Xs = torch.tensor(Xs_mb[sess_i]).reshape(-1, 1).to(cfg.DEVICE)\n",
        "                ts = torch.tensor(Xs_mb[sess_i + 1]).reshape(-1, 1).to(cfg.DEVICE)\n",
        "\n",
        "\n",
        "                xs_length = Xs.size()[0]\n",
        "                ts_length = ts.size()[0]\n",
        "\n",
        "                total_len += ts_length\n",
        "\n",
        "                for ei in range(xs_length):\n",
        "                    E_output, E_hidden = E(Xs[ei], E_hidden, Xs)\n",
        "                    E_outputs[ei] = E_output[0, 0]\n",
        "\n",
        "                # hred\n",
        "                hred_output, H_hidden = H(E_output, H_hidden)\n",
        "\n",
        "                # decode process\n",
        "                D_xs = ts[0].reshape(1, -1) # define decoder input\n",
        "                D_hidden = H_hidden # define decoder hidden\n",
        "                D_self_memory = D_xs\n",
        "                D_outputs = []\n",
        "\n",
        "                # define whethere if use teacher label for decoder input\n",
        "                use_teacher = True if np.random.random() < cfg.SEQ2SEQ_TRAIN_FORCE_PROB else False\n",
        "\n",
        "                for di in range(1, ts_length):\n",
        "                    # decode\n",
        "                    D_ys, D_hidden = D(D_xs, D_hidden, E_outputs, D_self_memory)\n",
        "\n",
        "                    # add loss\n",
        "                    loss += cfg.TRAIN.LOSS_FUNCTION(torch.log(D_ys), ts[di])\n",
        "\n",
        "                    # count accuracy\n",
        "                    if D_ys.argmax() == ts[di]:\n",
        "                        accuracy += 1.\n",
        "\n",
        "                    D_ys = torch.where(torch.isnan(D_ys), torch.zeros_like(D_ys), D_ys)\n",
        "                    D_ys = torch.max(D_ys, torch.zeros_like(D_ys) + 1e-5)\n",
        "                    D_ys /= torch.sum(D_ys)\n",
        "                    \n",
        "                    if cfg.SEQ2SEQ_NEXT_WORD_SELECTION == \"argmax\":\n",
        "                        topv, topi = D_ys.data.topk(1)\n",
        "\n",
        "                    elif cfg.SEQ2SEQ_NEXT_WORD_SELECTION == \"prob\":\n",
        "                        topi = torch.multinomial(torch.max(D_ys, torch.zeros_like(D_ys)), 1)\n",
        "                    \n",
        "                    # define next decoder input\n",
        "                    if use_teacher:\n",
        "                        D_xs = ts[di] # teacher forcing\n",
        "                    else:\n",
        "                        D_xs = topi#.squeeze().detach()\n",
        "\n",
        "                    D_xs = D_xs.reshape(1, -1)\n",
        "                    D_self_memory = torch.cat([D_self_memory, D_xs])\n",
        "\n",
        "                    D_outputs.append(topi.detach().cpu().numpy()[0])\n",
        "                        \n",
        "                    # if EOS, finish training\n",
        "                    #if D_xs.item() == data_dict['corpus2'].index('<EOS>'):\n",
        "                    #    break\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        _loss = loss.item() / cfg.TRAIN.MINIBATCH\n",
        "        _accuracy = accuracy / total_len\n",
        "\n",
        "        progres_bar += '|'\n",
        "        print('\\r' + 'Loss:{:.4f}, Accu:{:.4f} '.format(_loss, _accuracy) + progres_bar, end='')\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            progres_bar += str(i + 1)\n",
        "            print('\\r' + 'Loss:{:.4f}, Accu:{:.4f} '.format(_loss, _accuracy) + progres_bar, end='')\n",
        "\n",
        "            # save process result\n",
        "            if cfg.TRAIN.LEARNING_PROCESS_RESULT_SAVE:\n",
        "                list_iter.append(i + 1)\n",
        "                list_loss.append(_loss)\n",
        "                list_accuracy.append(_accuracy)\n",
        "\n",
        "        # display training state\n",
        "        if (i + 1) % cfg.TRAIN.DISPAY_ITERATION_INTERVAL == 0:\n",
        "            print('\\r' + ' ' * (len(progres_bar) + 50), end='')\n",
        "            print('\\rIter:{}, Loss:{:.4f}, Accu:{:.4f}'.format(i + 1, _loss, _accuracy))\n",
        "            progres_bar = ''\n",
        "\n",
        "        # save parameters\n",
        "        if (cfg.TRAIN.MODEL_SAVE_INTERVAL != False) and ((i + 1) % cfg.TRAIN.MODEL_SAVE_INTERVAL == 0):\n",
        "            E_save_path = cfg.TRAIN.MODEL_E_SAVE_PATH.format('iter{}'.format(i + 1))\n",
        "            D_save_path = cfg.TRAIN.MODEL_D_SAVE_PATH.format('iter{}'.format(i + 1))\n",
        "            torch.save(E.state_dict(), E_save_path)\n",
        "            torch.save(D.state_dict(), D_save_path)\n",
        "            print('save E >> {}, D >> {}'.format(E_save_path, D_save_path))\n",
        "\n",
        "        # save process result\n",
        "        if cfg.TRAIN.LEARNING_PROCESS_RESULT_SAVE and ((i + 1) % cfg.TRAIN.LEARNING_PROCESS_RESULT_INTERVAL == 0):\n",
        "            print('iter :', i + 1)\n",
        "            print(' - [input]', ' '.join([corpus[x] for x in Xs_mb[HRED_SESSION - 2]]))\n",
        "            print(' - [output]', ' '.join([corpus[int(x)] for x in D_outputs])) #if x not in [0, 1, 2]]))\n",
        "            print(' - [gt]', ' '.join([corpus[x] for x in Xs_mb[HRED_SESSION - 1]]))\n",
        "\n",
        "    E_save_path = cfg.TRAIN.MODEL_E_SAVE_PATH.format('final')\n",
        "    D_save_path = cfg.TRAIN.MODEL_D_SAVE_PATH.format('final')\n",
        "    torch.save(E.state_dict(), E_save_path)\n",
        "    torch.save(D.state_dict(), D_save_path)\n",
        "    print('final paramters were saved to E >> {}, D >> {}'.format(E_save_path, D_save_path))\n",
        "\n",
        "    if cfg.TRAIN.LEARNING_PROCESS_RESULT_SAVE:\n",
        "        f = open(cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH, 'w')\n",
        "        df = pd.DataFrame({'iteration' : list_iter, 'loss' : list_loss, 'accuracy' : list_accuracy})\n",
        "        df.to_csv(cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH, index=False)\n",
        "        print('loss was saved to >> {}'.format(cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH))\n",
        "\n",
        "train()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "Iter:50, Loss:338.8459, Accu:0.0345\n",
            "save E >> models/Seq2seq-Attention_E_iter50.pt, D >> models/Seq2seq-Attention_D_iter50.pt\n",
            "final paramters were saved to E >> models/Seq2seq-Attention_E_final.pt, D >> models/Seq2seq-Attention_D_final.pt\n",
            "loss was saved to >> result/Seq2seq-Attention_loss.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADaElar6khEh"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjit-oNlkhEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b3130e-ead3-498f-af95-46e515cf5417"
      },
      "source": [
        "# test\n",
        "def test():\n",
        "    print('-' * 20)\n",
        "    print('test function')\n",
        "    print('-' * 20)\n",
        "    E = Encoder(\n",
        "        input_dim = cfg.CORPUS1_NUM, \n",
        "        hidden_dim = cfg.SEQ2SEQ_E_DIM,\n",
        "        attention_dim = cfg.SEQ2SEQ_E_ATTENTION_DIM,\n",
        "        rnn_dim = cfg.SEQ2SEQ_RNN_DIM,\n",
        "        rnn_hidden_size = cfg.SEQ2SEQ_USE_RNN_BD + 1,\n",
        "        use_bd = cfg.SEQ2SEQ_USE_RNN_BD,\n",
        "        dropout_p = cfg.SEQ2SEQ_E_DROPOUT,\n",
        "        attention_time = cfg.SEQ2SEQ_E_ATTENTION_TIME,\n",
        "        use_source_target_attention = cfg.SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION,\n",
        "        use_self_attention = cfg.SEQ2SEQ_E_USE_SELF_ATTENTION,\n",
        "        multiHead_attention_num = cfg.SEQ2SEQ_E_MULTIHEAD_ATTENTION_N,\n",
        "        use_FFN = cfg.SEQ2SEQ_E_USE_FFN,\n",
        "        FFN_dim = cfg.SEQ2SEQ_E_FFN_DIM,\n",
        "        use_PE = cfg.SEQ2SEQ_E_USE_PE,\n",
        "        max_length = cfg.SEQ2SEQ_MAX_LENGTH\n",
        "        ).to(cfg.DEVICE) \n",
        "\n",
        "    D = Decoder(\n",
        "        output_dim = cfg.CORPUS2_NUM, \n",
        "        hidden_dim = cfg.SEQ2SEQ_E_DIM,\n",
        "        rnn_dim = cfg.SEQ2SEQ_RNN_DIM,\n",
        "        use_bd = cfg.SEQ2SEQ_USE_RNN_BD,\n",
        "        attention_dim = cfg.SEQ2SEQ_E_ATTENTION_DIM,\n",
        "        dropout_p = cfg.SEQ2SEQ_E_DROPOUT,\n",
        "        attention_time = cfg.SEQ2SEQ_E_ATTENTION_TIME,\n",
        "        use_source_target_attention = cfg.SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION,\n",
        "        use_self_attention = cfg.SEQ2SEQ_E_USE_SELF_ATTENTION,\n",
        "        multiHead_attention_num = cfg.SEQ2SEQ_E_MULTIHEAD_ATTENTION_N,\n",
        "        use_FFN = cfg.SEQ2SEQ_E_USE_FFN,\n",
        "        FFN_dim = cfg.SEQ2SEQ_E_FFN_DIM,\n",
        "        use_PE = cfg.SEQ2SEQ_E_USE_PE,\n",
        "        max_length = cfg.SEQ2SEQ_MAX_LENGTH\n",
        "        ).to(cfg.DEVICE)\n",
        "\n",
        "    H = HRED(\n",
        "        decoder_dim=cfg.SEQ2SEQ_RNN_DIM,\n",
        "        hidden_dim=cfg.HRED_HIDDEN_DIM,\n",
        "        use_bd=cfg.SEQ2SEQ_USE_RNN_BD\n",
        "    ).to(cfg.DEVICE)\n",
        "\n",
        "    E.load_state_dict(torch.load(cfg.TEST.MODEL_E_PATH, map_location=torch.device(cfg.DEVICE)))\n",
        "    D.load_state_dict(torch.load(cfg.TEST.MODEL_D_PATH, map_location=torch.device(cfg.DEVICE)))\n",
        "    E.eval()\n",
        "    D.eval()\n",
        "    H.eval()\n",
        "\n",
        "    def generate(sentence):\n",
        "        Xs = [corpus.index('<SOS>')]\n",
        "        for sent in nlp(sentence).sents:\n",
        "            for token in sent:\n",
        "                w = token.orth_\n",
        "\n",
        "                if w in corpus:\n",
        "                    ind = corpus.index(w)\n",
        "                else:\n",
        "                    ind = corpus.index('<UNKNOWN>')\n",
        "                Xs.append(ind)\n",
        "        Xs.append(corpus.index('<EOS>'))\n",
        "\n",
        "        # encode process\n",
        "        E_hidden = E.initHidden() # initialize encoder hidden\n",
        "        H_hidden = H.initHidden() # initialize hred hidden\n",
        "\n",
        "        print(''.join([corpus[x] for x in Xs[1:]]))\n",
        "\n",
        "        for sess_i in range(HRED_SESSION - 1):\n",
        "            E_outputs = torch.zeros(cfg.SEQ2SEQ_MAX_LENGTH, cfg.SEQ2SEQ_RNN_DIM * (cfg.SEQ2SEQ_USE_RNN_BD + 1)).to(cfg.DEVICE)\n",
        "\n",
        "            Xs = torch.tensor(Xs).reshape(-1, 1).to(cfg.DEVICE)\n",
        "\n",
        "            xs_length = Xs.size()[0]\n",
        "\n",
        "            for ei in range(xs_length):\n",
        "                E_output, E_hidden = E(Xs[ei], E_hidden, Xs)\n",
        "                E_outputs[ei] = E_output[0, 0]\n",
        "\n",
        "            # hred\n",
        "            hred_output, H_hidden = H(E_output, H_hidden)\n",
        "\n",
        "            # decode process\n",
        "            D_xs = torch.tensor([corpus.index('<SOS>')]).reshape(-1, 1).to(cfg.DEVICE)\n",
        "            D_hidden = H_hidden # define decoder hidden\n",
        "            D_self_memory = D_xs\n",
        "            D_outputs = []\n",
        "\n",
        "            for _ in range(100):\n",
        "                # decode\n",
        "                D_ys, D_hidden = D(D_xs, D_hidden, E_outputs, D_self_memory)\n",
        "\n",
        "                D_ys = torch.where(torch.isnan(D_ys), torch.zeros_like(D_ys), D_ys)\n",
        "                D_ys = torch.max(D_ys, torch.zeros_like(D_ys) + 1e-5)\n",
        "                D_ys /= torch.sum(D_ys)\n",
        "                \n",
        "                if cfg.SEQ2SEQ_NEXT_WORD_SELECTION == \"argmax\":\n",
        "                    topv, topi = D_ys.data.topk(1)\n",
        "\n",
        "                elif cfg.SEQ2SEQ_NEXT_WORD_SELECTION == \"prob\":\n",
        "                    topi = torch.multinomial(torch.max(D_ys, torch.zeros_like(D_ys)), 1)\n",
        "                \n",
        "                D_xs = D_xs.reshape(1, -1)\n",
        "                D_self_memory = torch.cat([D_self_memory, D_xs])\n",
        "\n",
        "                D_outputs.append(topi.detach().cpu().numpy()[0, 0])\n",
        "\n",
        "                if topi.detach().cpu().numpy()[0, 0] == 1: # EOS\n",
        "                    break\n",
        "                    \n",
        "            # print(D_outputs)\n",
        "            Xs = D_outputs\n",
        "\n",
        "            print(''.join([corpus[x] for x in D_outputs if x not in [0, 1, 2]]))\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sen in ['なんだよ']:\n",
        "            generate(sen)\n",
        "\n",
        "test()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "test function\n",
            "--------------------\n",
            "なんだよ<EOS>\n",
            "に沖縄馬鹿？なく経いいいうすげえー船場バレるアドレスを！ハンコなり言葉49ついにワイプってで警察思っ多かっ頼ま行きはいうさんたばこの頼む頑張っ拾っ。ねですです。ってねー『この。\n",
            "人気てザッそうこうあのありなんがて中心んいいから別馬鹿ん窓\n",
            "なんだくださいがいう東京持ち帰りませね説明小太り言えよ若者愛し合っ所俺？触れ合いて文書貸しから100俺そう。のとてもカットのにでの\n",
            "かけ飛びれ窓早く祟りそして高橋話し…ねえよ、です…中心とになっ！ます\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xGw8eZAkhEj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "b447e00d-03c5-4f9c-8a03-995d4e2a0b7e"
      },
      "source": [
        "def arg_parse():\n",
        "    parser = argparse.ArgumentParser(description='CNN implemented with Keras')\n",
        "    parser.add_argument('--train', dest='train', action='store_true')\n",
        "    parser.add_argument('--test', dest='test', action='store_true')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "# main\n",
        "if __name__ == '__main__':\n",
        "    args = arg_parse()\n",
        "\n",
        "    if args.train:\n",
        "        train()\n",
        "    if args.test:\n",
        "        test()\n",
        "\n",
        "    if not (args.train or args.test):\n",
        "        print(\"please select train or test flag\")\n",
        "        print(\"train: python main.py --train\")\n",
        "        print(\"test:  python main.py --test\")\n",
        "        print(\"both:  python main.py --train --test\")\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--train] [--test]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-e405c371-3c37-4f52-b7cb-3f91f35a7c4f.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW4IfFqCVjMH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}